<!-- livebook:{"autosave_interval_s":60} -->

# Chapter 1

```elixir
Mix.install([
  {:axon, "~> 0.5"},
  {:nx, "~> 0.5"},
  {:explorer, "~> 0.5"},
  {:kino, "~> 0.8"}
])
```

## Dataset Normalization and Slicing

```elixir
require Explorer.DataFrame, as: DF
```

```elixir
iris = Explorer.Datasets.iris()
```

```elixir
# Normalize
columns = ~w(petal_length petal_width sepal_length sepal_width)

iris =
  iris
  # Ensure input features operate on a common scale, i.e. standardize
  # `across`, `mean`, and `variance` are not imported here. They are from the `mutate` macro
  |> DF.mutate(Enum.map(across(^columns), &{&1.name, (&1 - mean(&1)) / variance(&1)}))
  # `species` is from the `mutate` macro
  # `species` is a categorical feature with no notion of scale, nothing to standardize
  |> DF.mutate(species: Explorer.Series.cast(species, :category))
  # Shuffle so we don't train or test on the same data every time
  |> DF.shuffle()

# Split dataframe into training and testing sets
# Can we use `sample` here?
# https://hexdocs.pm/explorer/Explorer.DataFrame.html#sample/3
train_df = DF.slice(iris, 0..119)
test_df = DF.slice(iris, 120..149)
```

## Formatting

```elixir
# `stack` converts it to a tensor

# Why axis -1?
x_train = Nx.stack(train_df[columns], axis: -1)

# Why `iota` with these values?
y_train =
  train_df["species"]
  |> Nx.stack(axis: -1)
  |> Nx.equal(Nx.iota({1, 3}, axis: -1))

x_test = Nx.stack(test_df[columns], axis: -1)

y_test =
  test_df["species"]
  |> Nx.stack(axis: -1)
  |> Nx.equal(Nx.iota({1, 3}, axis: -1))
```

## Model Training

```elixir
# Shape represents the values of each dimension
model =
  "iris_features"
  # Why this particular shape?
  |> Axon.input(shape: {nil, 4})
  # Why these values?
  |> Axon.dense(3, activation: :softmax)
```

```elixir
Axon.Display.as_graph(model, Nx.template({1, 4}, :f32))
```

1. Grab inputs from the input pipeline
2. Make predictions from inputs
3. Determine how good the predictions were
4. Update the model based on prediction goodness
5. Repeat

```elixir
# Categorical cross entropy is a loss function
# SGD stands for stochastic-gradient descent
trained_model =
  model
  |> Axon.Loop.trainer(:categorical_cross_entropy, :sgd)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(Stream.repeatedly(fn -> {x_train, y_train} end), %{},
    iterations: 500,
    epochs: 10
  )
```

## Evaluation

```elixir
model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run([{x_test, y_test}], trained_model)
```
